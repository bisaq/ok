{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bisaq/ok/blob/main/CSC3160_2024_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2\n",
        "\n",
        "In this assignment, we will focus on regular expressions and byte-pair encoding. Assignment 2 is worth 10% toward your final grade."
      ],
      "metadata": {
        "id": "f6iyutoXsYxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [50 marks] Regular expression\n",
        "\n",
        "1. [30marks] Write python code with regular expressions to clean the html webpage.\n",
        "\n",
        "  Input: https://slpcourse.github.io/materials/html_page.txt\n",
        "\n",
        "  Reference Output: https://slpcourse.github.io/materials/reference.txt\n",
        "\n",
        "2. [20 marks] Based on the output from 1, extract the lines with lecture time, tutorial time and office hours. Your need to use regular expressions.\n",
        "\n",
        "\n",
        "Note: We expect the regular expressions to be just enough for the task. If there are extra non-used regular expressions, we will deduct scores based the lines of non-used regular expressions. Each line that contains non-used regular expressions is worth 5 marks. Each uncleaned html tag is worth 2 points. Each unnecessary whitespace is worth 1 point."
      ],
      "metadata": {
        "id": "TT3nsKxz2UbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code here\n",
        "import re\n",
        "import requests\n",
        "\n",
        "def fetch_html(url):\n",
        "    \"\"\"Fetch HTML content from the given URL.\"\"\"\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raise an error for bad responses\n",
        "    return response.text\n",
        "\n",
        "def clean_html(html_content):\n",
        "    \"\"\"Remove HTML tags and clean up the HTML content.\"\"\"\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<[^>]+>', '', html_content)\n",
        "    text = re.sub(r'^\\s+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # 去除空行\n",
        "    text = re.sub(r'(?m)^[ \\t]*\\r?\\n', '', text)\n",
        "\n",
        "    # Remove extra whitespace, newlines, and tabs\n",
        "    # text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def save_clean_text(filename, text):\n",
        "    \"\"\"Save the cleaned text to a file.\"\"\"\n",
        "    with open(filename, 'w') as file:\n",
        "        file.write(text)\n",
        "\n",
        "url = 'https://slpcourse.github.io/materials/html_page.txt'\n",
        "output_file = 'cleaned_text.txt'\n",
        "\n",
        "# Fetch HTML content\n",
        "html_content = fetch_html(url)\n",
        "\n",
        "# Clean HTML content\n",
        "clean_text = clean_html(html_content)\n",
        "\n",
        "# Save cleaned text\n",
        "save_clean_text(output_file, clean_text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FjKVgSWFtmwq"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [50 marks] Byte-pair encoding\n",
        "\n",
        "In this assignment, the task is to implement a Byte-Pair Encoding (BPE) algorithm to learn subword tokens.\n",
        "\n",
        "Here is a vocabulary with frequency,\n",
        "```\n",
        "2 oneumonoultramicroscopicsilicovolcanoconiosis\n",
        "5 hippopotomonstrosesquippedaliophobia\n",
        "4 supercalifragilisticexpialidocious\n",
        "1 pseudopseudohypoparathyroidism\n",
        "6 floccinaucinihilipilification\n",
        "2 antidisestablishmentarianism\n",
        "5 honorificabilitudinitatibus\n",
        "```\n",
        "The first column represents the occurency frequency, and the second column represents the words.\n",
        "\n",
        "In the implementation, k is set to be 5."
      ],
      "metadata": {
        "id": "8TD1kIfo1emC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code here"
      ],
      "metadata": {
        "id": "_L6aAKCs2Rdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, defaultdict\n",
        "\n",
        "def get_stats(vocab):\n",
        "    \"\"\"Get frequency of pairs of adjacent symbols.\"\"\"\n",
        "    pairs = defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pair = (symbols[i], symbols[i + 1])\n",
        "            pairs[pair] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_vocab(pair, vocab):\n",
        "    \"\"\"Merge the most frequent pair in the vocabulary.\"\"\"\n",
        "    bigram = ' '.join(pair)\n",
        "    replacement = ''.join(pair)\n",
        "    new_vocab = {}\n",
        "    for word, freq in vocab.items():\n",
        "        new_word = word.replace(bigram, replacement)\n",
        "        new_vocab[new_word] = freq\n",
        "    return new_vocab\n",
        "\n",
        "def bpe(vocab, num_merges):\n",
        "    \"\"\"Perform Byte-Pair Encoding algorithm.\"\"\"\n",
        "    # Initial vocabulary: convert each word to a sequence of characters\n",
        "    vocab = { ' '.join(list(word)): freq for word, freq in vocab.items() }\n",
        "\n",
        "\n",
        "    for i in range(num_merges):\n",
        "        pairs = get_stats(vocab)\n",
        "        if not pairs:\n",
        "            break\n",
        "        # Find the most frequent pair\n",
        "        most_frequent_pair = max(pairs, key=pairs.get)\n",
        "        # Merge the most frequent pair\n",
        "        vocab = merge_vocab(most_frequent_pair, vocab)\n",
        "        print(f\"Merge {i + 1}: {most_frequent_pair}\")\n",
        "\n",
        "    return vocab\n",
        "\n",
        "# Define the initial vocabulary\n",
        "vocab = {\n",
        "    'oneumonoultramicroscopicsilicovolcanoconiosis': 2,\n",
        "    'hippopotomonstrosesquippedaliophobia': 5,\n",
        "    'supercalifragilisticexpialidocious': 4,\n",
        "    'pseudopseudohypoparathyroidism': 1,\n",
        "    'floccinaucinihilipilification': 6,\n",
        "    'antidisestablishmentarianism': 2,\n",
        "    'honorificabilitudinitatibus': 5\n",
        "}\n",
        "\n",
        "# Perform BPE with k = 5\n",
        "num_merges = 5\n",
        "final_vocab = bpe(vocab, num_merges)\n",
        "\n",
        "# Print the final vocabulary\n",
        "print(\"\\nFinal Vocabulary:\")\n",
        "for word, freq in final_vocab.items():\n",
        "    print(f\"{freq} {word}\")\n"
      ],
      "metadata": {
        "id": "NYzimXmYsG5E",
        "outputId": "a5216667-0408-4020-f3af-e48355902222",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'o n e u m o n o u l t r a m i c r o s c o p i c s i l i c o v o l c a n o c o n i o s i s': 2, 'h i p p o p o t o m o n s t r o s e s q u i p p e d a l i o p h o b i a': 5, 's u p e r c a l i f r a g i l i s t i c e x p i a l i d o c i o u s': 4, 'p s e u d o p s e u d o h y p o p a r a t h y r o i d i s m': 1, 'f l o c c i n a u c i n i h i l i p i l i f i c a t i o n': 6, 'a n t i d i s e s t a b l i s h m e n t a r i a n i s m': 2, 'h o n o r i f i c a b i l i t u d i n i t a t i b u s': 5}\n",
            "Merge 1: ('l', 'i')\n",
            "Merge 2: ('i', 'li')\n",
            "Merge 3: ('o', 'n')\n",
            "Merge 4: ('i', 'c')\n",
            "Merge 5: ('i', 'n')\n",
            "\n",
            "Final Vocabulary:\n",
            "2 on e u m on o u l t r a m ic r o s c o p ic s ilic o v o l c a n o c on i o s i s\n",
            "5 h i p p o p o t o m on s t r o s e s q u i p p e d a li o p h o b i a\n",
            "4 s u p e r c a li f r a g ili s t ic e x p i a li d o c i o u s\n",
            "1 p s e u d o p s e u d o h y p o p a r a t h y r o i d i s m\n",
            "6 f l o c c in a u c in i h ili p ili f ic a t i on\n",
            "2 a n t i d i s e s t a b li s h m e n t a r i a n i s m\n",
            "5 h on o r i f ic a b ili t u d in i t a t i b u s\n"
          ]
        }
      ]
    }
  ]
}